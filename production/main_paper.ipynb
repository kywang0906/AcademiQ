{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addb617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from constants import *\n",
    "from constants_id import *\n",
    "from indexing import BasicInvertedIndex\n",
    "from document_preprocessor import RegexTokenizer\n",
    "from ranker import Ranker, BM25\n",
    "from l2r import L2RFeatureExtractor, L2RRanker\n",
    "from relevance import map_score, ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9254c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199860/199860 [00:04<00:00, 44539.22it/s]\n",
      "100%|██████████| 45129/45129 [00:20<00:00, 2182.56it/s] \n"
     ]
    }
   ],
   "source": [
    "document_preprocessor = RegexTokenizer('\\\\w+')\n",
    "stopwords = set()\n",
    "with open(STOPWORD_PATH, \"r\") as f:\n",
    "    for word in f:\n",
    "        stopwords.add(word.strip())\n",
    "\n",
    "title_index = BasicInvertedIndex()\n",
    "title_index.load(PAPER_TITLE_INDEX)\n",
    "abstract_index = BasicInvertedIndex()\n",
    "abstract_index.load(PAPER_ABSTRACT_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc07dd1-9def-4903-840f-13aacb53bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load docid list\")\n",
    "# with open(DOCID_LIST_PATH, 'rb') as f:\n",
    "#     docid_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load categories\")\n",
    "# with open(DOC_CATEGORY_INFO_PATH, 'rb') as f:\n",
    "#     doc_category_info = pickle.load(f)\n",
    "# with open(RECOG_CATEGORY_PATH, 'rb') as f:\n",
    "#     recognized_categories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031d95e-d8cd-41fd-93cc-0bed4ad08ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load year release\")\n",
    "# with open(DOCID_TO_YEAR_RELEASE_PATH, 'rb') as f:\n",
    "#     docid_to_yr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708951c-623d-451e-916d-c4228c9b8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load citation\")\n",
    "# with open(DOCID_TO_CITATION_PATH, 'rb') as f:\n",
    "#     docid_to_citation = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fe4be-95b9-4ca9-9217-e3cd17baf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load network features\")\n",
    "# with open(DOCID_TO_NETWORK_FEATURES_PATH, 'rb') as f:\n",
    "#     docid_to_network_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09864c8-a4ba-4194-bd08-47a39f842e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 131051/6404472 [00:06<04:52, 21453.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3576590/415729739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAPER_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOTAL_PAPER_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_citation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docid_list = []\n",
    "with open(PAPER_DATA_PATH, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f, total=TOTAL_PAPER_COUNT)):\n",
    "        doc = json.loads(line)\n",
    "        if doc['abstract'] == '' or doc['n_citation'] <=20:\n",
    "            continue\n",
    "        \n",
    "        docid_list.append(doc['id'])\n",
    "\n",
    "with open(DOCID_LIST_PATH, 'wb') as f:\n",
    "    pickle.dump(docid_list, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995a9e40-80a4-41eb-b10f-6c6b76e3e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load categories\n",
      "Load year release\n",
      "Load citation\n",
      "Load network features\n"
     ]
    }
   ],
   "source": [
    "print(\"Load categories\")\n",
    "with open(ID_CATEGORY_INFO_PATH, 'rb') as f:\n",
    "    doc_category_info = pickle.load(f)\n",
    "with open(RECOG_CATEGORY_PATH, 'rb') as f:\n",
    "    recognized_categories = pickle.load(f)\n",
    "print(\"Load year release\")\n",
    "with open(ID_TO_YEAR_RELEASE_PATH, 'rb') as f:\n",
    "    docid_to_yr = pickle.load(f)\n",
    "print(\"Load citation\")\n",
    "with open(ID_TO_CITATION_PATH, 'rb') as f:\n",
    "    docid_to_citation = pickle.load(f)\n",
    "print(\"Load network features\")\n",
    "with open(ID_TO_NETWORK_FEATURES_PATH, 'rb') as f:\n",
    "    docid_to_network_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449c2753-e5e8-4a5e-a8f1-dbdd4d307431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Feature Extractor\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Feature Extractor\")\n",
    "feature_extractor = L2RFeatureExtractor(abstract_index, title_index,\n",
    "                doc_category_info, document_preprocessor, stopwords,\n",
    "                recognized_categories, docid_to_network_features, docid_to_yr, docid_to_citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ab4580-6495-4784-b637-9ec36dfb99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ranker\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Ranker\")\n",
    "BM25scorer = BM25(abstract_index)\n",
    "BM25Ranker = Ranker(abstract_index, document_preprocessor, stopwords, BM25scorer)\n",
    "\n",
    "l2rRanker = L2RRanker(document_preprocessor, stopwords, BM25Ranker, feature_extractor)\n",
    "\n",
    "# with open(BM25_RANKER_PATH, 'wb') as f:\n",
    "#     pickle.dump(BM25Ranker, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(L2R_RANKER_PATH, 'wb') as f:\n",
    "#     pickle.dump(l2rRanker, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934fa2bd-7032-4932-b0a7-c41e58a45fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'split', 'n_estimators': 20, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5887.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:54<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.5092554354503779\n",
      "Iteration:  1\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'split', 'n_estimators': 20, 'max_depth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5898.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:48<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.493414045244047\n",
      "Iteration:  2\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'split', 'n_estimators': 50, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5882.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:48<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.5119943292331592\n",
      "Iteration:  3\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'split', 'n_estimators': 50, 'max_depth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5869.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:53<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.4933150767078196\n",
      "Iteration:  4\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'gain', 'n_estimators': 20, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5907.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:50<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.5092554354503779\n",
      "Iteration:  5\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'gain', 'n_estimators': 20, 'max_depth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5906.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:51<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.493414045244047\n",
      "Iteration:  6\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'gain', 'n_estimators': 50, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5898.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:51<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.5119943292331592\n",
      "Iteration:  7\n",
      "{'boosting_type': 'gbdt', 'importance_type': 'gain', 'n_estimators': 50, 'max_depth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5868.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:52<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean NDCG:  0.4933150767078196\n",
      "Iteration:  8\n",
      "{'boosting_type': 'rf', 'importance_type': 'split', 'n_estimators': 20, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5748.06it/s]\n",
      "[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /__w/1/s/lightgbm-python/src/boosting/rf.hpp, line 36 .\n",
      "\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /__w/1/s/lightgbm-python/src/boosting/rf.hpp, line 36 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3868279/3717001293.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0ml2rRanker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0ml2rRanker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/paper_train_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mmap_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/si699/v16/l2r.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data_filename)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_to_document_relevance_scores_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/si699/v16/l2r.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, qgroups_train, eval_at)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m         self.ranker.fit(\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3435\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0mparams_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_param_dict_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3437\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[0m\u001b[1;32m   3438\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m                 \u001b[0m_c_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \"\"\"\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /__w/1/s/lightgbm-python/src/boosting/rf.hpp, line 36 .\n"
     ]
    }
   ],
   "source": [
    "id_col = 'docid'\n",
    "test_rel_df = pd.read_csv(\"dataset/paper_test_data.csv\")\n",
    "query_list = test_rel_df['query'].unique()\n",
    "\n",
    "boosting_type_list = [\"gbdt\", \"rf\"]\n",
    "importance_type_list = [\"split\", \"gain\"]\n",
    "n_estimators_list = [20, 50]\n",
    "max_depth_list = [3, 8]\n",
    "result_all = dict()\n",
    "for i, boosting_type in enumerate(boosting_type_list):\n",
    "    for j, importance_type in enumerate(importance_type_list):\n",
    "        for k, n_estimators in enumerate(n_estimators_list):\n",
    "            for l, max_depth in enumerate(max_depth_list):\n",
    "                params = {\n",
    "                    'boosting_type': boosting_type,\n",
    "                    'importance_type': importance_type,\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                }\n",
    "                it = i*8 + j*4 + k*2 + l\n",
    "                result_all[it] = params\n",
    "                print(\"Iteration: \", it)\n",
    "                print(params)\n",
    "                l2rRanker.model.re_init(params=params)\n",
    "                l2rRanker.train(\"dataset/paper_train_data.csv\")\n",
    "\n",
    "                map_list = []\n",
    "                ndcg_list = []\n",
    "                for query in tqdm(query_list):\n",
    "                    rank_result = l2rRanker.query(query)\n",
    "                    actual_rel = []\n",
    "                    rel_selected_df = test_rel_df[test_rel_df['query'] == query]\n",
    "                    rel_docid_list = list(rel_selected_df[id_col])\n",
    "\n",
    "                    for result in rank_result:\n",
    "                        if result[0] in rel_docid_list:\n",
    "                            actual_rel.append(rel_selected_df[rel_selected_df[id_col] == result[0]].iloc[0][\"rel\"])\n",
    "                        else:\n",
    "                            actual_rel.append(1)\n",
    "\n",
    "                    ideal_rel = sorted(actual_rel, reverse=True)\n",
    "                    ndcg_list.append(ndcg_score(actual_rel, ideal_rel))\n",
    "                print(\"mean NDCG: \", np.mean(ndcg_list))\n",
    "                result_all[it]['mean_ndcg'] = np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337f510f-a265-4f58-b225-78a0e7a08329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 20, 'max_depth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4520/4520 [00:00<00:00, 5378.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2490\n",
      "[LightGBM] [Info] Number of data points in the train set: 4520, number of used features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [03:24<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/paper_train_data.csv\n",
      "mean NDCG:  0.39899261914284045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [02:54<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/paper_test_data.csv\n",
      "mean NDCG:  0.5092554354503779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_col = 'docid'\n",
    "params = {'n_estimators': 20, 'max_depth': 3}\n",
    "print(params)\n",
    "l2rRanker.model.re_init(params=params)\n",
    "l2rRanker.train(\"dataset/paper_train_data.csv\")\n",
    "\n",
    "for path in [\"dataset/paper_train_data.csv\", \"dataset/paper_test_data.csv\"]:\n",
    "    test_rel_df = pd.read_csv(path)\n",
    "    query_list = test_rel_df['query'].unique()\n",
    "    map_list = []\n",
    "    ndcg_list = []\n",
    "    for query in tqdm(query_list):\n",
    "        rank_result = l2rRanker.query(query)\n",
    "        actual_rel = []\n",
    "        rel_selected_df = test_rel_df[test_rel_df['query'] == query]\n",
    "        rel_docid_list = list(rel_selected_df[id_col])\n",
    "\n",
    "        for result in rank_result:\n",
    "            if result[0] in rel_docid_list:\n",
    "                actual_rel.append(rel_selected_df[rel_selected_df[id_col] == result[0]].iloc[0][\"rel\"])\n",
    "            else:\n",
    "                actual_rel.append(1)\n",
    "\n",
    "        ideal_rel = sorted(actual_rel, reverse=True)\n",
    "        ndcg_list.append(ndcg_score(actual_rel, ideal_rel))\n",
    "    print(path)\n",
    "    print(\"mean NDCG: \", np.mean(ndcg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7cfabca-8f02-46b1-855e-f267ec772fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5650/5650 [00:01<00:00, 5623.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2498\n",
      "[LightGBM] [Info] Number of data points in the train set: 5650, number of used features: 14\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 20, 'max_depth': 3}\n",
    "\n",
    "with open(L2R_RANKER_PATH, 'rb') as f:\n",
    "    l2rRanker = pickle.load(f)\n",
    "\n",
    "l2rRanker.train(\"dataset/paper_all_data.csv\")\n",
    "with open(L2R_RANKER_FITTED_PATH, 'wb') as f:\n",
    "    pickle.dump(l2rRanker, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63adece8-992b-46b2-ae36-b899629bdc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data_df = pd.read_csv(f'{SCRACTCH_PATH}/paper_author_org/paper_level_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9dfe2f-1fc7-45ed-b24e-d10dcd222895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>author</th>\n",
       "      <th>org</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>docid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196907</th>\n",
       "      <td>1295745</td>\n",
       "      <td>1295745</td>\n",
       "      <td>Kernelized Subspace Ranking For Saliency Detec...</td>\n",
       "      <td>In this paper, we propose a novel saliency met...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Tiantian Wang; Lihe Zhang; Huchuan Lu; Chong S...</td>\n",
       "      <td>Dalian Univ Technol, Sch Informat &amp; Commun Eng...</td>\n",
       "      <td>102</td>\n",
       "      <td>4222231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96017</th>\n",
       "      <td>632899</td>\n",
       "      <td>632899</td>\n",
       "      <td>The Singular Value Decomposition, Applications...</td>\n",
       "      <td>The singular value decomposition (SVD) is not ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>zhihua zhang</td>\n",
       "      <td>-</td>\n",
       "      <td>47</td>\n",
       "      <td>2062995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145139</th>\n",
       "      <td>953281</td>\n",
       "      <td>953281</td>\n",
       "      <td>Motor imagery based brain-computer interface: ...</td>\n",
       "      <td>This article contains a new method to improvin...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Said Abenna; Mohammed Nahid; Abderrahim Bajit</td>\n",
       "      <td>Hassan II Univ, Fac Sci &amp; Technol, Casablanca,...</td>\n",
       "      <td>44</td>\n",
       "      <td>3108240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237365</th>\n",
       "      <td>1561782</td>\n",
       "      <td>1561782</td>\n",
       "      <td>Sign rank versus VC dimension</td>\n",
       "      <td>This work studies the maximum possible sign ra...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Noga Mordechai Alon; shay moran; amir yehudayoff</td>\n",
       "      <td>External Organizations; Algorithms and Complex...</td>\n",
       "      <td>40</td>\n",
       "      <td>5090405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31290</th>\n",
       "      <td>206073</td>\n",
       "      <td>206073</td>\n",
       "      <td>Adaptive affinity matrix learning for dimensio...</td>\n",
       "      <td>Conventional graph-based dimensionality reduct...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Junran He; Xiaozhao Fang; Peipei Kang; Lin Jia...</td>\n",
       "      <td>School of Computer Science and Technology, Gua...</td>\n",
       "      <td>42</td>\n",
       "      <td>671793.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235039</th>\n",
       "      <td>1546398</td>\n",
       "      <td>1546398</td>\n",
       "      <td>Cognitive Diversity: A Measurement of Dissimil...</td>\n",
       "      <td>In the context of computing and informatics, C...</td>\n",
       "      <td>2019</td>\n",
       "      <td>D. Frank Hsu; Bruce S. Kristal; Yuhan Hao; Chr...</td>\n",
       "      <td>Fordham Univ, Dept Comp &amp; Informat Sci, Lab In...</td>\n",
       "      <td>37</td>\n",
       "      <td>5040528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44749</th>\n",
       "      <td>294343</td>\n",
       "      <td>294343</td>\n",
       "      <td>A Low Rank Structural Large Margin Method For ...</td>\n",
       "      <td>Cross-modal retrieval is a classic research to...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Xinyan Lu; Fei Wu; Siliang Tang; Zhongfei Zhan...</td>\n",
       "      <td>Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejia...</td>\n",
       "      <td>54</td>\n",
       "      <td>958947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139511</th>\n",
       "      <td>917163</td>\n",
       "      <td>917163</td>\n",
       "      <td>Hyperspectral image denoising with bilinear lo...</td>\n",
       "      <td>•A bilinear low rank matrix factorization (BLR...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Huixin Fan; Jie Li; Qiangqiang Yuan; Xinxin Li...</td>\n",
       "      <td>School of Geodesy and Geomatics, Wuhan Univers...</td>\n",
       "      <td>43</td>\n",
       "      <td>2991144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21082</th>\n",
       "      <td>141004</td>\n",
       "      <td>141004</td>\n",
       "      <td>Nonconvex Regularizations for Feature Selectio...</td>\n",
       "      <td>Feature selection in learning to rank has rece...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Léa Laporte; Rémi Flamary; Stéphane Canu; Séba...</td>\n",
       "      <td>Univ Toulouse, Inst Rech Informat Toulouse, CN...</td>\n",
       "      <td>106</td>\n",
       "      <td>459294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37960</th>\n",
       "      <td>249546</td>\n",
       "      <td>249546</td>\n",
       "      <td>Fuzzy Ranking: Theory And Applications</td>\n",
       "      <td>The rank ordering of samples is widely used in...</td>\n",
       "      <td>2000</td>\n",
       "      <td>A Flaig; Ke Barner; Gr Arce</td>\n",
       "      <td>Univ Delaware, Dept Elect &amp; Comp Engn, Newark,...</td>\n",
       "      <td>25</td>\n",
       "      <td>813818.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  \\\n",
       "196907       1295745     1295745   \n",
       "96017         632899      632899   \n",
       "145139        953281      953281   \n",
       "237365       1561782     1561782   \n",
       "31290         206073      206073   \n",
       "235039       1546398     1546398   \n",
       "44749         294343      294343   \n",
       "139511        917163      917163   \n",
       "21082         141004      141004   \n",
       "37960         249546      249546   \n",
       "\n",
       "                                                    title  \\\n",
       "196907  Kernelized Subspace Ranking For Saliency Detec...   \n",
       "96017   The Singular Value Decomposition, Applications...   \n",
       "145139  Motor imagery based brain-computer interface: ...   \n",
       "237365                      Sign rank versus VC dimension   \n",
       "31290   Adaptive affinity matrix learning for dimensio...   \n",
       "235039  Cognitive Diversity: A Measurement of Dissimil...   \n",
       "44749   A Low Rank Structural Large Margin Method For ...   \n",
       "139511  Hyperspectral image denoising with bilinear lo...   \n",
       "21082   Nonconvex Regularizations for Feature Selectio...   \n",
       "37960              Fuzzy Ranking: Theory And Applications   \n",
       "\n",
       "                                                 abstract  year  \\\n",
       "196907  In this paper, we propose a novel saliency met...  2016   \n",
       "96017   The singular value decomposition (SVD) is not ...  2015   \n",
       "145139  This article contains a new method to improvin...  2022   \n",
       "237365  This work studies the maximum possible sign ra...  2016   \n",
       "31290   Conventional graph-based dimensionality reduct...  2023   \n",
       "235039  In the context of computing and informatics, C...  2019   \n",
       "44749   Cross-modal retrieval is a classic research to...  2013   \n",
       "139511  •A bilinear low rank matrix factorization (BLR...  2019   \n",
       "21082   Feature selection in learning to rank has rece...  2015   \n",
       "37960   The rank ordering of samples is widely used in...  2000   \n",
       "\n",
       "                                                   author  \\\n",
       "196907  Tiantian Wang; Lihe Zhang; Huchuan Lu; Chong S...   \n",
       "96017                                        zhihua zhang   \n",
       "145139      Said Abenna; Mohammed Nahid; Abderrahim Bajit   \n",
       "237365   Noga Mordechai Alon; shay moran; amir yehudayoff   \n",
       "31290   Junran He; Xiaozhao Fang; Peipei Kang; Lin Jia...   \n",
       "235039  D. Frank Hsu; Bruce S. Kristal; Yuhan Hao; Chr...   \n",
       "44749   Xinyan Lu; Fei Wu; Siliang Tang; Zhongfei Zhan...   \n",
       "139511  Huixin Fan; Jie Li; Qiangqiang Yuan; Xinxin Li...   \n",
       "21082   Léa Laporte; Rémi Flamary; Stéphane Canu; Séba...   \n",
       "37960                         A Flaig; Ke Barner; Gr Arce   \n",
       "\n",
       "                                                      org  n_citation  \\\n",
       "196907  Dalian Univ Technol, Sch Informat & Commun Eng...         102   \n",
       "96017                                                   -          47   \n",
       "145139  Hassan II Univ, Fac Sci & Technol, Casablanca,...          44   \n",
       "237365  External Organizations; Algorithms and Complex...          40   \n",
       "31290   School of Computer Science and Technology, Gua...          42   \n",
       "235039  Fordham Univ, Dept Comp & Informat Sci, Lab In...          37   \n",
       "44749   Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejia...          54   \n",
       "139511  School of Geodesy and Geomatics, Wuhan Univers...          43   \n",
       "21082   Univ Toulouse, Inst Rech Informat Toulouse, CN...         106   \n",
       "37960   Univ Delaware, Dept Elect & Comp Engn, Newark,...          25   \n",
       "\n",
       "            docid  \n",
       "196907  4222231.0  \n",
       "96017   2062995.0  \n",
       "145139  3108240.0  \n",
       "237365  5090405.0  \n",
       "31290    671793.0  \n",
       "235039  5040528.0  \n",
       "44749    958947.0  \n",
       "139511  2991144.0  \n",
       "21082    459294.0  \n",
       "37960    813818.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'learning to rank lightgbm'\n",
    "query_number = 10\n",
    "\n",
    "rank_result_list = l2rRanker.query(query)\n",
    "rank_result_df = pd.DataFrame(rank_result_list, columns=['docid','score'])\n",
    "rank_result_df = pd.merge(paper_data_df, rank_result_df)\n",
    "rank_result_df = rank_result_df.sort_values('score', ascending=False)[:query_number].drop(columns='score')\n",
    "rank_result_df = rank_result_df.fillna('-')\n",
    "rank_result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
